import os
import time
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack
from stable_baselines3.common.callbacks import CheckpointCallback

from envs.game_env import MHWildsEnv

def train():
    # 1. åˆ›å»ºç›®å½•ç”¨äºå­˜æ”¾æ¨¡å‹å’Œæ—¥å¿—
    timestamp = int(time.time())
    models_dir = f"models/PPO-{timestamp}"
    log_dir = "logs"

    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    print(f"æ¨¡å‹å°†ä¿å­˜åœ¨: {models_dir}")
    print(f"Tensorboard æ—¥å¿—å°†ä¿å­˜åœ¨: {log_dir}")

    # 2. åˆå§‹åŒ–ç¯å¢ƒ
    # ä½¿ç”¨ DummyVecEnv åŒ…è£…ï¼ŒSB3 è¦æ±‚å‘é‡åŒ–ç¯å¢ƒ
    # lambda å‡½æ•°ç”¨äºå»¶è¿Ÿåˆ›å»ºç¯å¢ƒå®ä¾‹
    env = DummyVecEnv([lambda: MHWildsEnv()])
    
    # å †å  4 å¸§ï¼Œè¿™æ · Agent èƒ½æ„ŸçŸ¥è¿åŠ¨æ–¹å‘å’Œé€Ÿåº¦
    # channels_order='last' å¯¹åº” (H, W, C)
    env = VecFrameStack(env, n_stack=4, channels_order='last')

    # 3. åˆå§‹åŒ–æ¨¡å‹
    # policy="CnnPolicy" ä¸“é—¨ç”¨äºå¤„ç†å›¾åƒè¾“å…¥
    model = PPO(
        "CnnPolicy", 
        env, 
        verbose=1, 
        tensorboard_log=log_dir,
        learning_rate=0.0003,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        device="cuda", # å¼ºåˆ¶ä½¿ç”¨ GPU
    )

    # 4. è®¾ç½®å›è°ƒå‡½æ•° (æ¯ 5000 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹)
    checkpoint_callback = CheckpointCallback(
        save_freq=5000, 
        save_path=models_dir, 
        name_prefix="mhwilds"
    )

    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ... (æŒ‰ Ctrl+C å¯ä»¥å®‰å…¨åœæ­¢å¹¶ä¿å­˜)")
    try:
        # è®­ç»ƒ 100ä¸‡æ­¥ (æ ¹æ®éœ€è¦è°ƒæ•´)
        model.learn(total_timesteps=1_000_000, callback=checkpoint_callback)
    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
    finally:
        model.save(f"{models_dir}/mhwilds_final")
        env.close()
        print("âœ… æ¨¡å‹å·²ä¿å­˜ï¼Œç¯å¢ƒå·²å…³é—­ã€‚")

if __name__ == "__main__":
    train()